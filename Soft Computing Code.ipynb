{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454b37e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /srv/conda/envs/notebook/lib/python3.6/site-packages (3.6.2)\n",
      "Requirement already satisfied: joblib in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: click in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: tqdm in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (4.61.1)\n",
      "Requirement already satisfied: importlib-metadata in /srv/conda/envs/notebook/lib/python3.6/site-packages (from click->nltk) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from importlib-metadata->click->nltk) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from importlib-metadata->click->nltk) (3.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2cd7953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99373326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Original Document: \n",
      "China net cafe culture crackdown\n",
      "\n",
      "Chinese authorities closed 12,575 net cafes in the closing months of 2004, the country's government said.\n",
      "\n",
      "According to the official news agency most of the net cafes were closed down because they were operating illegally. Chinese net cafes operate under a set of strict guidelines and\n",
      "many of those most recently closed broke rules that limit how close they can be to schools. The move is the latest in a series of steps the Chinese government has taken to crack\n",
      "down on what it considers to be immoral net use.\n",
      "\n",
      "The official Xinhua News Agency said the crackdown was carried out to create a \"safer environment for young people in China\". Rules introduced in 2002 demand that net cafes be at\n",
      "least 200 metres away from middle and elementary schools. The hours that children can use net cafes are also tightly regulated. China has long been worried that net cafes are an\n",
      "unhealthy influence on young people. The 12,575 cafes were shut in the three months from October to December. China also tries to dictate the types of computer games people can\n",
      "play to limit the amount of violence people are exposed to.\n",
      "\n",
      "Net cafes are hugely popular in China because the relatively high cost of computer hardware means that few people have PCs in their homes. This is not the first time that the\n",
      "Chinese government has moved against net cafes that are not operating within its strict guidelines. All the 100,000 or so net cafes in the country are required to use software\n",
      "that controls what websites users can see. Logs of sites people visit are also kept. Laws on net cafe opening hours and who can use them were introduced in 2002 following a fire\n",
      "at one cafe that killed 25 people. During the crackdown following the blaze authorities moved to clean up net cafes and demanded that all of them get permits to operate. In August\n",
      "2004 Chinese authorities shut down 700 websites and arrested 224 people in a crackdown on net porn. At the same time it introduced new controls to block overseas sex sites. The\n",
      "Reporters Without Borders group said in a report that Chinese government technologies for e-mail interception and net censorship are among the most highly developed in the world.\n",
      "\n",
      "\n",
      " \n",
      "Length of original document= 368  words\n",
      "The number of sentences in the original article= 19  \n"
     ]
    }
   ],
   "source": [
    "#Here we enter the raw or the original article\n",
    "DOCUMENT = \"\"\"\n",
    "China net cafe culture crackdown\n",
    "\n",
    "Chinese authorities closed 12,575 net cafes in the closing months of 2004, the country's government said.\n",
    "\n",
    "According to the official news agency most of the net cafes were closed down because they were operating illegally. Chinese net cafes operate under a set of strict guidelines and\n",
    "many of those most recently closed broke rules that limit how close they can be to schools. The move is the latest in a series of steps the Chinese government has taken to crack\n",
    "down on what it considers to be immoral net use.\n",
    "\n",
    "The official Xinhua News Agency said the crackdown was carried out to create a \"safer environment for young people in China\". Rules introduced in 2002 demand that net cafes be at\n",
    "least 200 metres away from middle and elementary schools. The hours that children can use net cafes are also tightly regulated. China has long been worried that net cafes are an\n",
    "unhealthy influence on young people. The 12,575 cafes were shut in the three months from October to December. China also tries to dictate the types of computer games people can\n",
    "play to limit the amount of violence people are exposed to.\n",
    "\n",
    "Net cafes are hugely popular in China because the relatively high cost of computer hardware means that few people have PCs in their homes. This is not the first time that the\n",
    "Chinese government has moved against net cafes that are not operating within its strict guidelines. All the 100,000 or so net cafes in the country are required to use software\n",
    "that controls what websites users can see. Logs of sites people visit are also kept. Laws on net cafe opening hours and who can use them were introduced in 2002 following a fire\n",
    "at one cafe that killed 25 people. During the crackdown following the blaze authorities moved to clean up net cafes and demanded that all of them get permits to operate. In August\n",
    "2004 Chinese authorities shut down 700 websites and arrested 224 people in a crackdown on net porn. At the same time it introduced new controls to block overseas sex sites. The\n",
    "Reporters Without Borders group said in a report that Chinese government technologies for e-mail interception and net censorship are among the most highly developed in the world.\n",
    "\n",
    "\"\"\";\n",
    "\n",
    "print(\"The Original Document:\",DOCUMENT)\n",
    "print(' ')\n",
    "L1=len(DOCUMENT)\n",
    "c0=0\n",
    "c1=0\n",
    "for i in range (0,L1):\n",
    "    if(DOCUMENT[i]==' '):\n",
    "        c1=c1+1\n",
    "    elif(DOCUMENT[i]=='.'):\n",
    "        c0=c0+1\n",
    "print(\"Length of original document=\",c1,\" words\")\n",
    "print(\"The number of sentences in the original article=\",c0,' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0be61f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #re is abuilt-in package which can be used to work with Regular Expressions. This module provides regular expression matching operations.\n",
    "DOCUMENT = re.sub(r'\\n|\\r', ' ', DOCUMENT)#removal of the line breaks or paragraph seperators\n",
    "DOCUMENT = re.sub(r' +', ' ', DOCUMENT)\n",
    "DOCUMENT = DOCUMENT.strip()#removes spaces at the begining and end of the input article\n",
    "sentences = nltk.sent_tokenize(DOCUMENT)#forms an array of sentences. These sentences arre those which are present in the input article seperated by a '.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb775473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The processed text:  \n",
      "['china net cafe culture crackdown chinese authorities closed net cafes closing months countrys government said'\n",
      " 'according official news agency net cafes closed operating illegally'\n",
      " 'chinese net cafes operate set strict guidelines many recently closed broke rules limit close schools'\n",
      " 'move latest series steps chinese government taken crack considers immoral net use'\n",
      " 'official xinhua news agency said crackdown carried create safer environment young people china'\n",
      " 'rules introduced demand net cafes least metres away middle elementary schools'\n",
      " 'hours children use net cafes also tightly regulated'\n",
      " 'china long worried net cafes unhealthy influence young people'\n",
      " 'cafes shut three months october december'\n",
      " 'china also tries dictate types computer games people play limit amount violence people exposed'\n",
      " 'net cafes hugely popular china relatively high cost computer hardware means people pcs homes'\n",
      " 'first time chinese government moved net cafes operating within strict guidelines'\n",
      " 'net cafes country required use software controls websites users see'\n",
      " 'logs sites people visit also kept'\n",
      " 'laws net cafe opening hours use introduced following fire one cafe killed people'\n",
      " 'crackdown following blaze authorities moved clean net cafes demanded get permits operate'\n",
      " 'august chinese authorities shut websites arrested people crackdown net porn'\n",
      " 'time introduced new controls block overseas sex sites'\n",
      " 'reporters without borders group said report chinese government technologies email interception net censorship among highly developed world']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "stop_words = nltk.corpus.stopwords.words('english')#returns the array of designated stopwords defined by nltk\n",
    "\n",
    "def normalize_document(doc):\n",
    "    \n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)#removes special characters (mainly the puncuation marks)\n",
    "    doc = doc.lower()#entire document is converted into lower case\n",
    "    doc = doc.strip()#removes whitespace from the beginning/end of the document\n",
    "    tokens = nltk.word_tokenize(doc)#tokenize the document. Returns the array of all the words present in the input article \n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]#filtering of the stopwords from the array of tokenized document\n",
    "    doc = ' '.join(filtered_tokens)# re-create document from filtered tokens\n",
    "    return doc\n",
    "normalize_corpus = np.vectorize(normalize_document)#normalization of the document\n",
    "\n",
    "norm_sentences = normalize_corpus(sentences)\n",
    "print(\"The processed text:\",' ')\n",
    "print(norm_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f05a5f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the value of 'm':10\n",
      "The selection matrix on vectorization for top 'm' words:,':  \n",
      "               0     1    2    3     4     5     6    7    8     9    10   11  \\\n",
      "according    0.00  0.41  0.0  0.0  0.00  0.00  0.00  0.0  0.0  0.00  0.0  0.0   \n",
      "agency       0.00  0.36  0.0  0.0  0.28  0.00  0.00  0.0  0.0  0.00  0.0  0.0   \n",
      "also         0.00  0.00  0.0  0.0  0.00  0.00  0.34  0.0  0.0  0.23  0.0  0.0   \n",
      "among        0.00  0.00  0.0  0.0  0.00  0.00  0.00  0.0  0.0  0.00  0.0  0.0   \n",
      "amount       0.00  0.00  0.0  0.0  0.00  0.00  0.00  0.0  0.0  0.29  0.0  0.0   \n",
      "arrested     0.00  0.00  0.0  0.0  0.00  0.00  0.00  0.0  0.0  0.00  0.0  0.0   \n",
      "august       0.00  0.00  0.0  0.0  0.00  0.00  0.00  0.0  0.0  0.00  0.0  0.0   \n",
      "authorities  0.26  0.00  0.0  0.0  0.00  0.00  0.00  0.0  0.0  0.00  0.0  0.0   \n",
      "away         0.00  0.00  0.0  0.0  0.00  0.34  0.00  0.0  0.0  0.00  0.0  0.0   \n",
      "blaze        0.00  0.00  0.0  0.0  0.00  0.00  0.00  0.0  0.0  0.00  0.0  0.0   \n",
      "\n",
      "              12    13   14    15    16   17    18  \n",
      "according    0.0  0.00  0.0  0.00  0.00  0.0  0.00  \n",
      "agency       0.0  0.00  0.0  0.00  0.00  0.0  0.00  \n",
      "also         0.0  0.36  0.0  0.00  0.00  0.0  0.00  \n",
      "among        0.0  0.00  0.0  0.00  0.00  0.0  0.26  \n",
      "amount       0.0  0.00  0.0  0.00  0.00  0.0  0.00  \n",
      "arrested     0.0  0.00  0.0  0.00  0.39  0.0  0.00  \n",
      "august       0.0  0.00  0.0  0.00  0.39  0.0  0.00  \n",
      "authorities  0.0  0.00  0.0  0.27  0.31  0.0  0.00  \n",
      "away         0.0  0.00  0.0  0.00  0.00  0.0  0.00  \n",
      "blaze        0.0  0.00  0.0  0.34  0.00  0.0  0.00  \n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer#Used for geerating TF-IDF scores\n",
    "import pandas as pd\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)#Convert a collection of sentences in the normalized document to a matrix of TF-IDF features\n",
    "dt_matrix = tv.fit_transform(norm_sentences)\n",
    "dt_matrix = dt_matrix.toarray()\n",
    "vocab = tv.get_feature_names()#Array of words (in the normalized document) sorted on the basis of their respective TF-IDF features\n",
    "td_matrix = dt_matrix.T#Transpose of matrix dt_matrix\n",
    "td_matrix.shape\n",
    "m=int(input(\"Enter the value of 'm' ( any number between 10-20 ):\"))\n",
    "print(\"The selection matrix on vectorization for top 'm' words:,':\",' ')\n",
    "print(pd.DataFrame(np.round(td_matrix, 2), index=vocab).head(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a686a6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of segments for summary (Enter 3 or 5 as an input):3\n",
      "Enter the number of sentences you require in the summary:8\n",
      "Saline scores:  \n",
      "[0.65271468 0.56394579 0.53252843 0.38211541 0.72249958 0.36200433\n",
      " 0.56475884 0.55658241 0.23711151 0.60158722 0.456053   0.60475891\n",
      " 0.44482743 0.58003323 0.5063153  0.42602088 0.44140093 0.42300557\n",
      " 0.30244031]\n"
     ]
    }
   ],
   "source": [
    "#Latent Semantic Analysis\n",
    "from scipy.sparse.linalg import svds   \n",
    "def low_rank_svd(matrix, singular_count=2):\n",
    "    u, s, vt = svds(matrix, k=singular_count)\n",
    "    return u, s, vt\n",
    "n=int(input(\"Enter the number of segments for summary (Enter 3 or 5 as an input):\"))\n",
    "num_sentences = int(input(\"Enter the number of sentences you require in the summary:\"))\n",
    "num_topics = n\n",
    "u, s, vt = low_rank_svd(td_matrix, singular_count=num_topics)  \n",
    "u.shape, s.shape, vt.shape\n",
    "term_topic_mat, singular_values, topic_document_mat = u, s, vt\n",
    "sv_threshold = 0.5\n",
    "min_sigma_value = max(singular_values) * sv_threshold\n",
    "singular_values[singular_values < min_sigma_value] = 0\n",
    "salience_scores = np.sqrt(np.dot(np.square(singular_values), np.square(topic_document_mat)))\n",
    "print(\"Saline scores:\",' ')\n",
    "print(salience_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f30ee074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "The similarity matrix:  \n",
      "[[1.    0.153 0.152 0.13  0.167 0.058 0.074 0.131 0.149 0.043 0.096 0.176\n",
      "  0.062 0.    0.182 0.185 0.238 0.    0.159]\n",
      " [0.153 1.    0.121 0.02  0.296 0.05  0.064 0.06  0.039 0.    0.044 0.172\n",
      "  0.054 0.    0.018 0.049 0.024 0.    0.016]\n",
      " [0.152 0.121 1.    0.052 0.    0.196 0.047 0.045 0.029 0.067 0.033 0.258\n",
      "  0.04  0.    0.014 0.115 0.063 0.    0.042]\n",
      " [0.13  0.02  0.052 1.    0.    0.017 0.095 0.02  0.    0.    0.015 0.128\n",
      "  0.08  0.    0.065 0.017 0.068 0.    0.089]\n",
      " [0.167 0.296 0.    0.    1.    0.    0.    0.202 0.    0.102 0.075 0.\n",
      "  0.    0.049 0.032 0.056 0.106 0.    0.052]\n",
      " [0.058 0.05  0.196 0.017 0.    1.    0.054 0.051 0.033 0.    0.037 0.046\n",
      "  0.045 0.    0.079 0.042 0.02  0.081 0.014]\n",
      " [0.074 0.064 0.047 0.095 0.    0.054 1.    0.065 0.042 0.079 0.048 0.059\n",
      "  0.141 0.125 0.187 0.053 0.026 0.    0.017]\n",
      " [0.131 0.06  0.045 0.02  0.202 0.051 0.065 1.    0.039 0.133 0.142 0.056\n",
      "  0.055 0.064 0.06  0.05  0.078 0.    0.016]\n",
      " [0.149 0.039 0.029 0.    0.    0.033 0.042 0.039 1.    0.    0.029 0.036\n",
      "  0.035 0.    0.    0.032 0.138 0.    0.   ]\n",
      " [0.043 0.    0.067 0.    0.102 0.    0.079 0.133 0.    1.    0.165 0.\n",
      "  0.    0.173 0.058 0.    0.076 0.    0.   ]\n",
      " [0.096 0.044 0.033 0.015 0.075 0.037 0.048 0.142 0.029 0.165 1.    0.041\n",
      "  0.04  0.047 0.044 0.037 0.058 0.    0.012]\n",
      " [0.176 0.172 0.258 0.128 0.    0.046 0.059 0.056 0.036 0.    0.041 1.\n",
      "  0.05  0.    0.017 0.143 0.079 0.11  0.104]\n",
      " [0.062 0.054 0.04  0.08  0.    0.045 0.141 0.055 0.035 0.    0.04  0.05\n",
      "  1.    0.    0.074 0.045 0.132 0.107 0.015]\n",
      " [0.    0.    0.    0.    0.049 0.    0.125 0.064 0.    0.173 0.047 0.\n",
      "  0.    1.    0.046 0.    0.06  0.134 0.   ]\n",
      " [0.182 0.018 0.014 0.065 0.032 0.079 0.187 0.06  0.    0.058 0.044 0.017\n",
      "  0.074 0.046 1.    0.093 0.057 0.071 0.012]\n",
      " [0.185 0.049 0.115 0.017 0.056 0.042 0.053 0.05  0.032 0.    0.037 0.143\n",
      "  0.045 0.    0.093 1.    0.171 0.    0.013]\n",
      " [0.238 0.024 0.063 0.068 0.106 0.02  0.026 0.078 0.138 0.076 0.058 0.079\n",
      "  0.132 0.06  0.057 0.171 1.    0.    0.055]\n",
      " [0.    0.    0.    0.    0.    0.081 0.    0.    0.    0.    0.    0.11\n",
      "  0.107 0.134 0.071 0.    0.    1.    0.   ]\n",
      " [0.159 0.016 0.042 0.089 0.052 0.014 0.017 0.016 0.    0.    0.012 0.104\n",
      "  0.015 0.    0.012 0.013 0.055 0.    1.   ]]\n"
     ]
    }
   ],
   "source": [
    "top_sentence_indices = (-salience_scores).argsort()[:num_sentences]\n",
    "print(top_sentence_indices.sort())\n",
    "similarity_matrix = np.matmul(dt_matrix, dt_matrix.T)\n",
    "similarity_matrix.shape\n",
    "print(\"The similarity matrix:\",' ')\n",
    "print(np.round(similarity_matrix, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64815305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "The summary:  \n",
      " \n",
      "China net cafe culture crackdown Chinese authorities closed 12,575 net cafes in the closing months of 2004, the country's government said.\n",
      "According to the official news agency most of the net cafes were closed down because they were operating illegally.\n",
      "Chinese net cafes operate under a set of strict guidelines and many of those most recently closed broke rules that limit how close they can be to schools.\n",
      "The hours that children can use net cafes are also tightly regulated.\n",
      "China has long been worried that net cafes are an unhealthy influence on young people.\n",
      "This is not the first time that the Chinese government has moved against net cafes that are not operating within its strict guidelines.\n",
      "Laws on net cafe opening hours and who can use them were introduced in 2002 following a fire at one cafe that killed 25 people.\n",
      "In August 2004 Chinese authorities shut down 700 websites and arrested 224 people in a crackdown on net porn.\n",
      " \n",
      "Length of summary= 162  words\n"
     ]
    }
   ],
   "source": [
    "#text rank sorting an the final summary \n",
    "import networkx\n",
    "similarity_graph = networkx.from_numpy_array(similarity_matrix)\n",
    "similarity_graph\n",
    "scores = networkx.pagerank(similarity_graph)\n",
    "ranked_sentences = sorted(((score, index) for index, score in scores.items()), reverse=True)\n",
    "ranked_sentences[:10]\n",
    "top_sentence_indices = [ranked_sentences[index][1] for index in range(num_sentences)]\n",
    "top_sentence_indices.sort()\n",
    "\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"The summary:\",' ')\n",
    "print(' ')\n",
    "final_summary='\\n'.join(np.array(sentences)[top_sentence_indices])\n",
    "print(final_summary)\n",
    "print(' ')\n",
    "L2=len(final_summary)\n",
    "c2=0\n",
    "for i in range (0,L2):\n",
    "    if(DOCUMENT[i]==' '):\n",
    "        c2=c2+1\n",
    "print(\"Length of summary=\",c2,\" words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab5f1db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'rouge'...\n",
      "remote: Enumerating objects: 269, done.\u001b[K\n",
      "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 269 (delta 7), reused 15 (delta 5), pack-reused 247\u001b[K\n",
      "Receiving objects: 100% (269/269), 71.66 KiB | 4.48 MiB/s, done.\n",
      "Resolving deltas: 100% (124/124), done.\n",
      "/home/jovyan/binder/rouge/rouge/rouge\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "creating rouge.egg-info\n",
      "writing rouge.egg-info/PKG-INFO\n",
      "writing dependency_links to rouge.egg-info/dependency_links.txt\n",
      "writing entry points to rouge.egg-info/entry_points.txt\n",
      "writing requirements to rouge.egg-info/requires.txt\n",
      "writing top-level names to rouge.egg-info/top_level.txt\n",
      "writing manifest file 'rouge.egg-info/SOURCES.txt'\n",
      "reading manifest file 'rouge.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'rouge.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "creating build/lib/rouge\n",
      "copying rouge/rouge_score.py -> build/lib/rouge\n",
      "copying rouge/__init__.py -> build/lib/rouge\n",
      "copying rouge/rouge.py -> build/lib/rouge\n",
      "creating build/lib/bin\n",
      "copying bin/__init__.py -> build/lib/bin\n",
      "copying bin/rouge_cmd.py -> build/lib/bin\n",
      "creating build/bdist.linux-x86_64\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/rouge\n",
      "copying build/lib/rouge/rouge_score.py -> build/bdist.linux-x86_64/egg/rouge\n",
      "copying build/lib/rouge/__init__.py -> build/bdist.linux-x86_64/egg/rouge\n",
      "copying build/lib/rouge/rouge.py -> build/bdist.linux-x86_64/egg/rouge\n",
      "creating build/bdist.linux-x86_64/egg/bin\n",
      "copying build/lib/bin/__init__.py -> build/bdist.linux-x86_64/egg/bin\n",
      "copying build/lib/bin/rouge_cmd.py -> build/bdist.linux-x86_64/egg/bin\n",
      "byte-compiling build/bdist.linux-x86_64/egg/rouge/rouge_score.py to rouge_score.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/rouge/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/rouge/rouge.py to rouge.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/bin/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/bin/rouge_cmd.py to rouge_cmd.cpython-36.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying rouge.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying rouge.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying rouge.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying rouge.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying rouge.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying rouge.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating dist\n",
      "creating 'dist/rouge-1.0.0rc2-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing rouge-1.0.0rc2-py3.6.egg\n",
      "Removing /srv/conda/envs/notebook/lib/python3.6/site-packages/rouge-1.0.0rc2-py3.6.egg\n",
      "Copying rouge-1.0.0rc2-py3.6.egg to /srv/conda/envs/notebook/lib/python3.6/site-packages\n",
      "rouge 1.0.0rc2 is already the active version in easy-install.pth\n",
      "Installing rouge script to /srv/conda/envs/notebook/bin\n",
      "\n",
      "Installed /srv/conda/envs/notebook/lib/python3.6/site-packages/rouge-1.0.0rc2-py3.6.egg\n",
      "Processing dependencies for rouge==1.0.0rc2\n",
      "Searching for six==1.15.0\n",
      "Best match: six 1.15.0\n",
      "Adding six 1.15.0 to easy-install.pth file\n",
      "\n",
      "Using /srv/conda/envs/notebook/lib/python3.6/site-packages\n",
      "Finished processing dependencies for rouge==1.0.0rc2\n"
     ]
    }
   ],
   "source": [
    "#Rouge score calculation. For Rouge score calculation, the human generated summary is needed for comparison with the proposed architecture generated one.\n",
    "\n",
    "!git clone https://github.com/pltrdy/rouge\n",
    "%cd rouge\n",
    "!python setup.py install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffa66298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'r': 0.98, 'p': 0.5, 'f': 0.6621621576880935}, 'rouge-2': {'r': 0.953125, 'p': 0.4066666666666667, 'f': 0.5700934537514194}, 'rouge-l': {'r': 0.98, 'p': 0.5, 'f': 0.6621621576880935}}]\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "#in order to measure the Rouge scores, it is neccessary to obtain the corresponding human summary\n",
    "human_summary=\"\"\" \n",
    "Chinese authorities closed 12,575 net cafes in the closing months of 2004, the country's government said.\n",
    "Chinese net cafes operate under a set of strict guidelines and many of those most recently closed broke rules that limit how close they can be to schools.\n",
    "This is not the first time that the Chinese government has moved against net cafes that are not operating within its strict guidelines.\n",
    "\"\"\";\n",
    "hypothesis =final_summary ;\n",
    "reference =human_summary\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3797a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity:  0.013986013986013986\n"
     ]
    }
   ],
   "source": [
    "#To measure the cosine similarity\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "X = DOCUMENT.lower() \n",
    "Y = final_summary.lower() \n",
    "\n",
    "X_list = word_tokenize(X)  \n",
    "Y_list = word_tokenize(Y) \n",
    "\n",
    "sw = stopwords.words('english')  \n",
    "l1 =[];l2 =[] \n",
    "\n",
    "X_set = {w for w in X_list if not w in sw}  \n",
    "Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "rvector = X_set.union(Y_set)  \n",
    "for w in rvector: \n",
    "    if w in X_set: l1.append(1)\n",
    "    else: l1.append(0) \n",
    "    if w in Y_set: l2.append(1) \n",
    "    else: l2.append(0) \n",
    "c = 0\n",
    "\n",
    "for i in range(len(rvector)): \n",
    "        c+= l1[i]*l2[i] \n",
    "cosine = c / float((sum(l1)*sum(l2))*0.5) \n",
    "print(\"Cosine Similarity: \", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e5ec94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting language-check\n",
      "  Using cached language-check-1.1.tar.gz (33 kB)\n",
      "Building wheels for collected packages: language-check\n",
      "  Building wheel for language-check (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /srv/conda/envs/notebook/bin/python3.6 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-dxhpn_v1\n",
      "       cwd: /tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/\n",
      "  Complete output (35 lines):\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/setup.py\", line 595, in <module>\n",
      "      sys.exit(main())\n",
      "    File \"/tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/setup.py\", line 590, in main\n",
      "      run_setup_hooks(config)\n",
      "    File \"/tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/setup.py\", line 561, in run_setup_hooks\n",
      "      language_tool_hook(config)\n",
      "    File \"/tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/setup.py\", line 584, in language_tool_hook\n",
      "      download_lt()\n",
      "    File \"/tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/download_lt.py\", line 128, in download_lt\n",
      "      with closing(urlopen(url)) as u:\n",
      "    File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 223, in urlopen\n",
      "      return opener.open(url, data, timeout)\n",
      "    File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 532, in open\n",
      "      response = meth(req, response)\n",
      "    File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 642, in http_response\n",
      "      'http', request, response, code, msg, hdrs)\n",
      "    File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 564, in error\n",
      "      result = self._call_chain(*args)\n",
      "    File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 504, in _call_chain\n",
      "      result = func(*args)\n",
      "    File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 756, in http_error_302\n",
      "      return self.parent.open(new, timeout=req.timeout)\n",
      "    File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 532, in open\n",
      "      response = meth(req, response)\n",
      "    File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 642, in http_response\n",
      "      'http', request, response, code, msg, hdrs)\n",
      "    File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 570, in error\n",
      "      return self._call_chain(*args)\n",
      "    File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 504, in _call_chain\n",
      "      result = func(*args)\n",
      "    File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 650, in http_error_default\n",
      "      raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "  urllib.error.HTTPError: HTTP Error 403: Forbidden\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for language-check\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for language-check\n",
      "Failed to build language-check\n",
      "Installing collected packages: language-check\n",
      "    Running setup.py install for language-check ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /srv/conda/envs/notebook/bin/python3.6 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-mh5_3p31/install-record.txt --single-version-externally-managed --compile --install-headers /srv/conda/envs/notebook/include/python3.6m/language-check\n",
      "         cwd: /tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/\n",
      "    Complete output (35 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/setup.py\", line 595, in <module>\n",
      "        sys.exit(main())\n",
      "      File \"/tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/setup.py\", line 590, in main\n",
      "        run_setup_hooks(config)\n",
      "      File \"/tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/setup.py\", line 561, in run_setup_hooks\n",
      "        language_tool_hook(config)\n",
      "      File \"/tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/setup.py\", line 584, in language_tool_hook\n",
      "        download_lt()\n",
      "      File \"/tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/download_lt.py\", line 128, in download_lt\n",
      "        with closing(urlopen(url)) as u:\n",
      "      File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 223, in urlopen\n",
      "        return opener.open(url, data, timeout)\n",
      "      File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 532, in open\n",
      "        response = meth(req, response)\n",
      "      File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 642, in http_response\n",
      "        'http', request, response, code, msg, hdrs)\n",
      "      File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 564, in error\n",
      "        result = self._call_chain(*args)\n",
      "      File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 504, in _call_chain\n",
      "        result = func(*args)\n",
      "      File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 756, in http_error_302\n",
      "        return self.parent.open(new, timeout=req.timeout)\n",
      "      File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 532, in open\n",
      "        response = meth(req, response)\n",
      "      File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 642, in http_response\n",
      "        'http', request, response, code, msg, hdrs)\n",
      "      File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 570, in error\n",
      "        return self._call_chain(*args)\n",
      "      File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 504, in _call_chain\n",
      "        result = func(*args)\n",
      "      File \"/srv/conda/envs/notebook/lib/python3.6/urllib/request.py\", line 650, in http_error_default\n",
      "        raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "    urllib.error.HTTPError: HTTP Error 403: Forbidden\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /srv/conda/envs/notebook/bin/python3.6 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-ao8ntm0f/language-check_1a933e9d83c54f5fa5862fcd4c0c54b1/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-mh5_3p31/install-record.txt --single-version-externally-managed --compile --install-headers /srv/conda/envs/notebook/include/python3.6m/language-check Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade language-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e47cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check the number of gramatical errors in the generated summary\n",
    "import language_check\n",
    "# Mention the language keyword\n",
    "tool = language_check.LanguageTool('en-US')\n",
    "i = 0\n",
    "# Path of file which needs to be checked\n",
    "with open(r'C:\\Users\\win10\\Desktop\\Summaries\\Tech-No.2.txt', 'r') as fin:            \n",
    "    for line in fin:\n",
    "        matches = tool.check(line)\n",
    "        i = i + len(matches)     \n",
    "        pass\n",
    "# prints total mistakes which are found \n",
    "# from the document\n",
    "print(\"No. of mistakes found in document is \", i)\n",
    "print() \n",
    "# prints mistake one by one \n",
    "for mistake in matches:\n",
    "    print(mistake)\n",
    "    print()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
